{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, mean_absolute_percentage_error, make_scorer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy.stats import uniform\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.experimental import enable_halving_search_cv \n",
    "from sklearn.model_selection import HalvingRandomSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../Models/Real_Estate_Model.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change data type of city, state, and zip_code to category\n",
    "df['city'] = df['city'].astype('category')\n",
    "df['state'] = df['state'].astype('category')\n",
    "df['zip_code'] = df['zip_code'].astype('category')\n",
    "df['bed'] = df['bed'].astype(int)\n",
    "df['bath'] = df['bath'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['price', 'house_size', 'bed', 'bath']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features (X) and target (y)\n",
    "X = df[['bath', 'bed', 'house_size', 'state', 'city']].copy()  # Explicitly create a copy\n",
    "y = df['price']  # Target variable\n",
    "\n",
    "# Encode categorical variables\n",
    "label_encoder_state = LabelEncoder()\n",
    "label_encoder_city = LabelEncoder()\n",
    "\n",
    "X['state'] = label_encoder_state.fit_transform(X['state'])\n",
    "X['city'] = label_encoder_city.fit_transform(X['city'])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the MLP model\n",
    "mlp = MLPRegressor(\n",
    "    hidden_layer_sizes=(64, 32),  \n",
    "    activation='relu',           \n",
    "    solver='adam',              \n",
    "    max_iter=500,     \n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.1,\n",
    "    n_iter_no_change=10,         \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "mlp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = mlp.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R^2 Score:\", r2)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "print(\"Mean Absolute Percentage Error:\", mape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = {\n",
    "    'hidden_layer_sizes': [(64, 32), (128, 64)],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'solver': ['adam', 'sgd'],\n",
    "    'alpha': uniform(0.01),\n",
    "    'learning_rate_init': uniform(0.01)\n",
    "}\n",
    "\n",
    "halving_search = HalvingRandomSearchCV(\n",
    "    estimator=mlp,\n",
    "    param_distributions=param_dist,\n",
    "    n_candidates=20,\n",
    "    scoring='neg_mean_absolute_percentage_error',\n",
    "    cv=2,\n",
    "    factor=3, \n",
    "    resource='n_samples',\n",
    "    max_resources=100000,\n",
    "    min_resources='smallest',\n",
    "    aggressive_elimination=True,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "halving_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Parameters:\", halving_search.best_params_)\n",
    "print(\"Best MAPE Score:\", abs(halving_search.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter tuning confirmed that the parameters used in the base model were optimal, as they were ultimately selected as the best. Since LGBM outperformed the other models when it comes to performance and efficiency, I will proceed with it for the app deployment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
